from pathlib import Path
from typing import List
import shutil
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

# ì„¸ì…˜ë³„ë¡œ ì±„ìš© ê³µê³  ë¬¸ë§¥ì„ ë²¡í„°DB(Chroma)ì— ì €ì¥/ì¡°íšŒ
ROOT = Path(__file__).resolve().parents[1]
SESS_DB_ROOT = ROOT / "db" / "sessions"
SESS_DB_ROOT.mkdir(parents=True, exist_ok=True)


def _load_docs(file_path: str):
    """
    íŒŒì¼ í™•ì¥ìì— ë”°ë¼ PDF/Text ë¡œë” ì„ íƒ
    """
    p = Path(file_path)
    if p.suffix.lower() == ".pdf":
        return PyPDFLoader(str(p)).load()
    return TextLoader(str(p), encoding="utf-8").load()


def create_or_reset_session(chat_id: str, job_file_path: str):
    """
    ì„¸ì…˜ ìƒì„±/ë¦¬ì…‹
    1) ì±„ìš© ê³µê³  íŒŒì¼ì„ ë¬¸ì„œ ì¡°ê°ìœ¼ë¡œ ë¶„í• 
    2) ì„ë² ë”© ìƒì„±
    3) ì„¸ì…˜ë³„ Chroma ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥(ì§€ì†)
    """
    docs = _load_docs(job_file_path)

    splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=200)
    chunks = splitter.split_documents(docs)
    for ch in chunks:
        ch.metadata = {"chat_id": chat_id}

    persist_dir = str(SESS_DB_ROOT / chat_id)

    # ğŸ”§ Eski sessiya fayllarini tozalash
    if Path(persist_dir).exists():
        shutil.rmtree(persist_dir, ignore_errors=True)

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    # ìµœì‹  Chromaì—ì„œëŠ” .persist() í˜¸ì¶œ ì—†ì´ë„ ìë™ ì§€ì† ì €ì¥
    Chroma.from_documents(chunks, embedding=embeddings, persist_directory=persist_dir)
    return {"chat_id": chat_id, "persist_dir": persist_dir}


def retrieve_job_context(chat_id: str, query: str = "Evaluate candidate against this job"):
    """
    ì„¸ì…˜ ë²¡í„°DBì—ì„œ ê´€ë ¨ ë¬¸ë§¥ ê²€ìƒ‰ í›„ ìƒìœ„ kê°œë¥¼ í•©ì³ ë°˜í™˜
    """
    persist_dir = str(SESS_DB_ROOT / chat_id)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    hits = db.similarity_search(query, k=4)
    return "\n\n".join([h.page_content for h in hits])


def end_session(chat_id: str):
    """
    ì„¸ì…˜ ì¢…ë£Œ ì‹œ ë””ìŠ¤í¬ ì •ë¦¬(ì„ íƒ)
    """
    dirp = SESS_DB_ROOT / chat_id
    if dirp.exists():
        shutil.rmtree(dirp, ignore_errors=True)
